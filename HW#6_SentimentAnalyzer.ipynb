{"cells":[{"metadata":{},"source":["# Dependencies\n","import tweepy\n","import json\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","SIA = SentimentIntensityAnalyzer()\n","from config import consumer_key, consumer_secret, access_token, access_token_secret\n","\n","# Setup Tweepy API Authentication\n","auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n","auth.set_access_token(access_token, access_token_secret)\n","api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())"],"cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{},"source":["# Target User\n","target_users = ['@bbcworld', '@cbsnews', '@cnn', '@foxnews', '@nytimes']\n","\n","# Set Lists\n","users_list = []\n","pos_list = []\n","neu_list = []\n","neg_list = []\n","compound_list = []\n","timestamp_list = []\n","text_list = []\n","tweet_count = []\n","sentiment_list = []\n","\n","#cycle through the different users\n","for target_user in target_users:\n","    users_list1 = []\n","    content_list1 = []\n","    pos_list1 = []\n","    neu_list1 = []\n","    neg_list1 = []\n","    compound_list1 = []\n","    timestamp_list1 = []\n","    text_list1 = []\n","    tweet_count1 = []\n","    tweetcount = 0\n","    \n","#start looking up each tweet - for loop range 5 so that 20 tweets per \n","    for i in range(5):\n","        tweets = api.user_timeline(target_user, page=i)\n","        \n","        for tweet in tweets:\n","            text = tweet[\"text\"]\n","            print(f\"{text}\")\n","            timestamp = tweet[\"created_at\"]\n","            pos = SIA.polarity_scores(tweet[\"text\"])[\"pos\"]\n","            neu = SIA.polarity_scores(tweet[\"text\"])[\"pos\"]\n","            neg = SIA.polarity_scores(tweet[\"text\"])[\"pos\"]\n","            compound = SIA.polarity_scores(tweet[\"text\"])[\"compound\"]\n","            users_list1.append(tweet[\"user\"][\"screen_name\"])\n","            timestamp_list1.append(timestamp)\n","            compound_list1.append(compound)\n","            pos_list1.append(pos)\n","            neu_list1.append(neu)\n","            neg_list1.append(neg)\n","            text_list1.append(text)\n","            tweetcount += 1\n","            tweet_count1.append(tweetcount)\n","    \n","    users_list.append(users_list1)\n","    pos_list.append(pos_list1)\n","    neu_list.append(neu_list1)\n","    neg_list.append(neg_list1)\n","    compound_list.append(compound_list1)\n","    timestamp_list.append(timestamp_list1)\n","    text_list.append(text_list1)\n","    tweet_count.append(tweet_count1)    \n","    \n","    \n","sentiments = np.mean(compound_list)\n","sentiment_list.append(sentiments)"],"cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{},"source":[],"cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{},"source":[],"cell_type":"code","execution_count":null,"outputs":[]}],"metadata":{"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"kernels_config":{"r":{"delete_cmd_postfix":") ","varRefreshCmd":"cat(var_dic_list()) ","delete_cmd_prefix":"rm(","library":"var_list.r"},"python":{"delete_cmd_postfix":"","varRefreshCmd":"print(var_dic_list())","delete_cmd_prefix":"del ","library":"var_list.py"}},"window_display":false},"language_info":{"file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text\/x-python","pygments_lexer":"ipython3","version":"3.7.3","nbconvert_exporter":"python","name":"python"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}},"nbformat_minor":2,"nbformat":4}